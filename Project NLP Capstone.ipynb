{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyM2WS6J8pc8+JXMF3Dn7/Gr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"102b82732e7b47529440f98639a39254":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_184cf3be2d234d17b7cdd2591e6adc6c","IPY_MODEL_000b34a4d9c24107856a63f8bf209f4e","IPY_MODEL_988b09e306c2423d98256a17136e69d8"],"layout":"IPY_MODEL_10586c8d7ef24d86bec14fc657fa1a43"}},"184cf3be2d234d17b7cdd2591e6adc6c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed0265124a1c4b4aa88e7e862077853a","placeholder":"​","style":"IPY_MODEL_990a4a66b1e34b2f83dcf7009c1188a1","value":"model.safetensors: 100%"}},"000b34a4d9c24107856a63f8bf209f4e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ee3b3ac0617e4422a622eb64b4d8b8e6","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c9ffbeafb32c4ec48a2d4fe20e74dcbf","value":440449768}},"988b09e306c2423d98256a17136e69d8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_53abea4a630a4bdab563ef99e9dd1d42","placeholder":"​","style":"IPY_MODEL_09b39b2fff3e48f0bcfa1ecfdc65f200","value":" 440M/440M [00:11&lt;00:00, 49.6MB/s]"}},"10586c8d7ef24d86bec14fc657fa1a43":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed0265124a1c4b4aa88e7e862077853a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"990a4a66b1e34b2f83dcf7009c1188a1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ee3b3ac0617e4422a622eb64b4d8b8e6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9ffbeafb32c4ec48a2d4fe20e74dcbf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"53abea4a630a4bdab563ef99e9dd1d42":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"09b39b2fff3e48f0bcfa1ecfdc65f200":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n","import tensorflow as tf\n","from transformers import BertTokenizer, BertForSequenceClassification\n","import torch\n","from torch.optim import AdamW\n","import importlib.metadata\n","\n","# Print library versions for debugging\n","print(\"TensorFlow version:\", tf.__version__)\n","print(\"Transformers version:\", importlib.metadata.version(\"transformers\"))\n","print(\"PyTorch version:\", torch.__version__)\n","\n","# Download required NLTK data\n","nltk.download('punkt', quiet=True)\n","nltk.download('stopwords', quiet=True)\n","nltk.download('wordnet', quiet=True)\n","nltk.download('punkt_tab', quiet=True)\n","\n","class SentimentAnalysisPipeline:\n","    def __init__(self):\n","        self.stop_words = set(stopwords.words('english'))\n","        self.lemmatizer = WordNetLemmatizer()\n","        self.vectorizer = TfidfVectorizer(max_features=5000)\n","        self.tokenizer = Tokenizer(num_words=5000)\n","        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    def clean_text(self, text):\n","        \"\"\"Clean and preprocess text data\"\"\"\n","        text = str(text)\n","        text = text.lower()\n","        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n","        tokens = word_tokenize(text)\n","        tokens = [self.lemmatizer.lemmatize(token) for token in tokens\n","                 if token not in self.stop_words]\n","        return ' '.join(tokens)\n","\n","    def load_and_preprocess_data(self, dataset_path):\n","        \"\"\"Load and preprocess the dataset\"\"\"\n","        df = pd.read_csv(dataset_path)\n","        print(\"DataFrame columns:\", df.columns)\n","\n","        if 'review_text' not in df.columns or 'sentiment' not in df.columns:\n","            raise ValueError(\"Dataset must contain 'review_text' and 'sentiment' columns\")\n","\n","        df['cleaned_review'] = df['review_text'].apply(self.clean_text)\n","        sentiment_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n","        df['label'] = df['sentiment'].map(sentiment_map)\n","\n","        if df['label'].isna().any():\n","            raise ValueError(\"Invalid sentiment labels found. Expected 'positive', 'neutral', or 'negative'.\")\n","\n","        return df\n","\n","    def prepare_data_for_traditional_models(self, df):\n","        \"\"\"Prepare data for Logistic Regression and Naive Bayes\"\"\"\n","        X = self.vectorizer.fit_transform(df['cleaned_review']).toarray()\n","        y = df['label'].values\n","        return train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","    def prepare_data_for_lstm(self, df, max_sequence_length=100):\n","        \"\"\"Prepare data for LSTM model\"\"\"\n","        self.tokenizer.fit_on_texts(df['cleaned_review'])\n","        sequences = self.tokenizer.texts_to_sequences(df['cleaned_review'])\n","        X = pad_sequences(sequences, maxlen=max_sequence_length)\n","        y = df['label'].values\n","        return train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","    def prepare_data_for_bert(self, df, max_length=128):\n","        \"\"\"Prepare data for BERT model\"\"\"\n","        input_ids = []\n","        attention_masks = []\n","        labels = df['label'].values\n","\n","        for review in df['cleaned_review']:\n","            encoded = self.bert_tokenizer.encode_plus(\n","                review,\n","                add_special_tokens=True,\n","                max_length=max_length,\n","                padding='max_length',\n","                truncation=True,\n","                return_attention_mask=True,\n","                return_tensors='pt'  # PyTorch tensors\n","            )\n","            input_ids.append(encoded['input_ids'][0])\n","            attention_masks.append(encoded['attention_mask'][0])\n","\n","        input_ids = torch.stack(input_ids)\n","        attention_masks = torch.stack(attention_masks)\n","        labels = torch.tensor(labels, dtype=torch.long)\n","\n","        dataset = torch.utils.data.TensorDataset(input_ids, attention_masks, labels)\n","        train_size = int(0.8 * len(dataset))\n","        test_size = len(dataset) - train_size\n","        train_dataset, test_dataset = torch.utils.data.random_split(\n","            dataset, [train_size, test_size], generator=torch.Generator().manual_seed(42)\n","        )\n","\n","        return train_dataset, test_dataset\n","\n","    def train_logistic_regression(self, X_train, X_test, y_train, y_test):\n","        \"\"\"Train and evaluate Logistic Regression model\"\"\"\n","        model = LogisticRegression(multi_class='multinomial', max_iter=1000)\n","        model.fit(X_train, y_train)\n","\n","        y_pred = model.predict(X_test)\n","        return {\n","            'accuracy': accuracy_score(y_test, y_pred),\n","            'f1_score': f1_score(y_test, y_pred, average='weighted'),\n","            'confusion_matrix': confusion_matrix(y_test, y_pred)\n","        }\n","\n","    def train_naive_bayes(self, X_train, X_test, y_train, y_test):\n","        \"\"\"Train and evaluate Naive Bayes model\"\"\"\n","        model = MultinomialNB()\n","        model.fit(X_train, y_train)\n","\n","        y_pred = model.predict(X_test)\n","        return {\n","            'accuracy': accuracy_score(y_test, y_pred),\n","            'f1_score': f1_score(y_test, y_pred, average='weighted'),\n","            'confusion_matrix': confusion_matrix(y_test, y_pred)\n","        }\n","\n","    def train_lstm(self, X_train, X_test, y_train, y_test, vocab_size=5000,\n","                  max_sequence_length=100):\n","        \"\"\"Train and evaluate LSTM model\"\"\"\n","        model = Sequential([\n","            Embedding(vocab_size, 100),\n","            LSTM(128, return_sequences=False),\n","            Dropout(0.2),\n","            Dense(64, activation='relu'),\n","            Dropout(0.2),\n","            Dense(3, activation='softmax')\n","        ])\n","\n","        model.compile(optimizer='adam',\n","                     loss='sparse_categorical_crossentropy',\n","                     metrics=['accuracy'])\n","\n","        model.fit(X_train, y_train, epochs=5, batch_size=32,\n","                 validation_split=0.2, verbose=0)\n","\n","        y_pred = np.argmax(model.predict(X_test, verbose=0), axis=1)\n","        return {\n","            'accuracy': accuracy_score(y_test, y_pred),\n","            'f1_score': f1_score(y_test, y_pred, average='weighted'),\n","            'confusion_matrix': confusion_matrix(y_test, y_pred)\n","        }\n","\n","    def train_bert(self, train_dataset, test_dataset, max_length=128):\n","        \"\"\"Train and evaluate BERT model\"\"\"\n","        model = BertForSequenceClassification.from_pretrained(\n","            'bert-base-uncased',\n","            num_labels=3\n","        ).to(self.device)\n","\n","        optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n","        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n","        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8)\n","\n","        # Training loop\n","        model.train()\n","        for epoch in range(3):\n","            total_loss = 0\n","            for batch in train_loader:\n","                input_ids, attention_mask, labels = [b.to(self.device) for b in batch]\n","                optimizer.zero_grad()\n","                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","                loss = outputs.loss\n","                total_loss += loss.item()\n","                loss.backward()\n","                optimizer.step()\n","            print(f\"Epoch {epoch+1}, Average Loss: {total_loss / len(train_loader):.4f}\")\n","\n","        # Evaluation\n","        model.eval()\n","        y_true = []\n","        y_pred = []\n","        with torch.no_grad():\n","            for batch in test_loader:\n","                input_ids, attention_mask, labels = [b.to(self.device) for b in batch]\n","                outputs = model(input_ids, attention_mask=attention_mask)\n","                preds = torch.argmax(outputs.logits, dim=1)\n","                y_true.extend(labels.cpu().numpy())\n","                y_pred.extend(preds.cpu().numpy())\n","\n","        y_true = np.array(y_true)\n","        y_pred = np.array(y_pred)\n","        return {\n","            'accuracy': accuracy_score(y_true, y_pred),\n","            'f1_score': f1_score(y_true, y_pred, average='weighted'),\n","            'confusion_matrix': confusion_matrix(y_true, y_pred)\n","        }\n","\n","    def plot_confusion_matrix(self, cm, model_name, save_path=None):\n","        \"\"\"Plot confusion matrix\"\"\"\n","        plt.figure(figsize=(8, 6))\n","        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n","        plt.title(f'Confusion Matrix - {model_name}')\n","        plt.ylabel('True Label')\n","        plt.xlabel('Predicted Label')\n","        if save_path:\n","            plt.savefig(save_path)\n","            plt.close()\n","        else:\n","            plt.show()\n","\n","def main():\n","    # Initialize pipeline\n","    pipeline = SentimentAnalysisPipeline()\n","\n","    # Load and preprocess data\n","    dataset_path = 'bike_rental_reviews.csv'  # Update with actual path\n","    df = pipeline.load_and_preprocess_data(dataset_path)\n","\n","    # Train and evaluate traditional models\n","    X_train_trad, X_test_trad, y_train_trad, y_test_trad = \\\n","        pipeline.prepare_data_for_traditional_models(df)\n","\n","    lr_results = pipeline.train_logistic_regression(\n","        X_train_trad, X_test_trad, y_train_trad, y_test_trad)\n","    nb_results = pipeline.train_naive_bayes(\n","        X_train_trad, X_test_trad, y_train_trad, y_test_trad)\n","\n","    # Train and evaluate LSTM\n","    X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm = \\\n","        pipeline.prepare_data_for_lstm(df)\n","    lstm_results = pipeline.train_lstm(\n","        X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm)\n","\n","    # Train and evaluate BERT\n","    train_dataset, test_dataset = pipeline.prepare_data_for_bert(df)\n","    bert_results = pipeline.train_bert(train_dataset, test_dataset)\n","\n","    # Print results\n","    print(\"Logistic Regression Results:\",\n","          f\"Accuracy: {lr_results['accuracy']:.4f}, F1-Score: {lr_results['f1_score']:.4f}\")\n","    print(\"Naive Bayes Results:\",\n","          f\"Accuracy: {nb_results['accuracy']:.4f}, F1-Score: {nb_results['f1_score']:.4f}\")\n","    print(\"LSTM Results:\",\n","          f\"Accuracy: {lstm_results['accuracy']:.4f}, F1-Score: {lstm_results['f1_score']:.4f}\")\n","    print(\"BERT Results:\",\n","          f\"Accuracy: {bert_results['accuracy']:.4f}, F1-Score: {bert_results['f1_score']:.4f}\")\n","\n","    # Plot confusion matrices\n","    pipeline.plot_confusion_matrix(lr_results['confusion_matrix'],\n","                                  'Logistic Regression', 'lr_cm.png')\n","    pipeline.plot_confusion_matrix(nb_results['confusion_matrix'],\n","                                  'Naive Bayes', 'nb_cm.png')\n","    pipeline.plot_confusion_matrix(lstm_results['confusion_matrix'],\n","                                  'LSTM', 'lstm_cm.png')\n","    pipeline.plot_confusion_matrix(bert_results['confusion_matrix'],\n","                                  'BERT', 'bert_cm.png')\n","\n","if __name__ == '__main__':\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":329,"referenced_widgets":["102b82732e7b47529440f98639a39254","184cf3be2d234d17b7cdd2591e6adc6c","000b34a4d9c24107856a63f8bf209f4e","988b09e306c2423d98256a17136e69d8","10586c8d7ef24d86bec14fc657fa1a43","ed0265124a1c4b4aa88e7e862077853a","990a4a66b1e34b2f83dcf7009c1188a1","ee3b3ac0617e4422a622eb64b4d8b8e6","c9ffbeafb32c4ec48a2d4fe20e74dcbf","53abea4a630a4bdab563ef99e9dd1d42","09b39b2fff3e48f0bcfa1ecfdc65f200"]},"id":"V5AbyygzqFpw","outputId":"7b9fd5c6-2894-4fa0-d743-9088f2a7eba1","executionInfo":{"status":"ok","timestamp":1755888112879,"user_tz":300,"elapsed":3065165,"user":{"displayName":"Ranjan Baral","userId":"07147690062707915284"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow version: 2.19.0\n","Transformers version: 4.55.2\n","PyTorch version: 2.8.0+cu126\n","DataFrame columns: Index(['review_text', 'sentiment'], dtype='object')\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"102b82732e7b47529440f98639a39254"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Average Loss: 0.0084\n","Epoch 2, Average Loss: 0.0026\n","Epoch 3, Average Loss: 0.0000\n","Logistic Regression Results: Accuracy: 1.0000, F1-Score: 1.0000\n","Naive Bayes Results: Accuracy: 1.0000, F1-Score: 1.0000\n","LSTM Results: Accuracy: 1.0000, F1-Score: 1.0000\n","BERT Results: Accuracy: 1.0000, F1-Score: 1.0000\n"]}]},{"cell_type":"markdown","source":["#Sentiment Analysis Model Evaluation Report\n","This report evaluates four models—Logistic Regression, Naive Bayes, LSTM, and BERT—trained on the bike_rental_reviews.csv dataset for sentiment analysis, using accuracy, F1-score, and confusion matrices. The dataset includes review_text and sentiment columns, with labels mapped as positive (2), neutral (1), and negative (0). Models were evaluated on a 20% test split in Google Colab (TensorFlow 2.19.0, transformers 4.55.2, PyTorch 2.8.0+cu126).\n","\n","1. Logistic Regression:\n","\n","\n","* Accuracy: 1.0000\n","* F1-Score: 1.0000 (weighted)\n","* Confusion Matrix: Saved as lr_cm.png. Perfect classification suggests overfitting or data issues (e.g., small dataset, leakage).\n","\n","2. Naive Bayes:\n","* Accuracy: 1.0000\n","* F1-Score: 1.0000 (weighted)\n","* Confusion Matrix: Saved as nb_cm.png. Perfect predictions indicate potential dataset or preprocessing problems.\n","\n","3. LSTM:\n","\n","* Accuracy: 1.0000\n","* F1-Score: 1.0000 (weighted)\n","* Confusion Matrix: Saved as lstm_cm.png. Perfect performance points to data leakage or insufficient complexity.\n","\n","4. BERT:\n","* Accuracy: 1.0000\n","* F1-Score: 1.0000 (weighted)\n","* Confusion Matrix: Saved as bert_cm.png. Perfect scores and 0.0000 loss by Epoch 3 suggest overfitting or label issues.\n","\n","# Observations:\n","\n","1. Perfect scores (1.0000) across all models are unrealistic, indicating a small dataset, data leakage, imbalanced labels, or preprocessing errors.\n","\n","2. Execution took >50 minutes, driven by BERT training (bert-base-uncased).\n"],"metadata":{"id":"QvU4d6Z-HR9Z"}}]}